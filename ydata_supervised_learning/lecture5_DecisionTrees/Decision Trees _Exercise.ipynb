{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cg047ujRBmtU"
   },
   "source": [
    "# Decision Trees Exercise\n",
    "In this exercise you will show that ID3 is sub-optimal. Implement a simple version of Decision Tree, and will then apply a Decision Tree classsifier on the MNIST hand written digits dataset that we already saw.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "osXAmT5y4iM8"
   },
   "source": [
    "## Suboptimality of ID3\n",
    "Consider the following training set, where $\\mathcal{X} = \\{0, 1\\}^3$ and $\\mathcal{Y} =\\{0, 1\\}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "((1, 1, 1), 1)\\\\\n",
    "((1, 0, 0), 1)\\\\\n",
    "((1, 1, 0), 0)\\\\\n",
    "((0, 0, 1), 0)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Suppose we wish to use this training set in order to build a decision tree of depth 2 (i.e. for each\n",
    "input we are allowed to ask two questions of the form \"$x_i = 0$?\" before deciding on the label).\n",
    "\n",
    "1. Suppose we run the ID3 algorithm up to depth 2 (namely, we pick the root node and its\n",
    "children according to the algorithm, but instead of keeping on with the recursion, we stop\n",
    "and pick leaves according to the majority label in each subtree, once we reach depth 2). \n",
    "Assume that the subroutine used to measure the quality of each feature is based on the information gain, and that if two features get the same score, one of them is picked arbitrarily. \n",
    "Show that the training error of the resulting decision tree is at least 1/4.\n",
    "2. Find a decision tree of depth 2, which attains zero training error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xC7Anlwu50XD"
   },
   "source": [
    "#### Answer\n",
    "Put your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tLXpoHg64HlD"
   },
   "source": [
    "## Implementing Decision Tree From Scratch\n",
    "In this exercise you will need to implement a simple version of Decision Tree from scratch. Your decision tree will handle **continuous input and output** (this should actually work also for binary input attributes).\n",
    "\n",
    "* Compelete the skeleton class below\n",
    "  - `X` is a matrix of data values (rows are samples, columns are attributes)\n",
    "  - `y` is a vector of corresponding target values\n",
    "  - `min_leaf` is the minimal number of samples in each leaf node\n",
    "  \n",
    "* For splitting criterion, use either **\"Train Squared Error Minimization (Reduction in Variance)\"** or **\"Train Absolute Error Minimization\"** (choose one). Whatever you choose, make sure you implement the splitting point decision efficiently (in $O(nlgn)$ time).\n",
    "\n",
    "* The `predict` function will use mean of the target values in the leaf node matching each row of the given `X`. The result is a vector of predictions matching the number of rows in `X`.\n",
    "\n",
    "* To check your decision tree implementation, use the boston dataset (`from sklearn.datasets import load_boston`) split the data set into train and test using (`from sklearn.model_selection import train_test_split`)\n",
    "\n",
    "  - Use the following to estimate what are the best hyper parameters to use for your model\n",
    "```\n",
    "    for min_leaf in [1,5,10,100]:\n",
    "      dt = DecisionTree(X, y, n, sz, min_leaf)\n",
    "      mse = # mean square error over test set\n",
    "      print(\"min_leaf:{0} --- oob mse: {1}\".format(min_leaf, mse))\n",
    "```\n",
    "  \n",
    "  - Using your chosen hyperparameters as a final model, plot the predictions vs. true values of all the samples in the training set . Use something like:\n",
    "  ```\n",
    "  y_hat = dt.predict(X_train)  # forest is the chosen model\n",
    "  plt.scatter(y_hat, y_test)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinarySearchNode():\n",
    "    def __init__(self, data):\n",
    "        self.data = data \n",
    "        self.left = None \n",
    "        self.right = None\n",
    "    \n",
    "    def add_child(self, data, decision_boundary):\n",
    "        self.decision_boundary = decision_boundary\n",
    "        if data == self.data: #Need to check the value: If the value already exists, we don't continue-- we don't want duplicates \n",
    "            return\n",
    "\n",
    "        if data < self.decision_boundary: #Condition: If new data is less than the parent node\n",
    "            if self.left: #Check that our left element is not a leaf node\n",
    "                self.left.add_child(data) #If it self.left has a child, add data as a child to self.left's child.\n",
    "            else:\n",
    "                self.left = BinarySearchNode(data) #If the left node is empty though, then it is already a leaf. We designate it as a node, with left and right = None.\n",
    "        else:\n",
    "            if self.right:\n",
    "                self.right.add_child(data)\n",
    "            else:\n",
    "                self.right = BinarySearchNode(data)\n",
    "    \n",
    "\n",
    "    def in_order_traversal(self): #SORTING ALGORITHM\n",
    "        elements = []\n",
    "        #We sort from smallest to largest, so start with left tree\n",
    "        if self.left:\n",
    "            elements += self.left.in_order_traversal() #This iterates until we get to the bottom most left node, adds that node, then recursively adds the previous ones to our list.\n",
    "        \n",
    "        #Once we are done adding each left node, we have reached the root. Now, we add the root:\n",
    "        elements.append(self.data)\n",
    "\n",
    "        if self.right:\n",
    "            elements += self.right.in_order_traversal()\n",
    "        \n",
    "        return elements\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(rows):\n",
    "    score, question = find_split(rows)\n",
    "\n",
    "    if score == 0: return Leaf(rows)\n",
    "\n",
    "    true_rows, false_rows = partition(rows, question)\n",
    "\n",
    "    true_branch = build_tree(true_rows)\n",
    "\n",
    "    false_branch = build_tree(false_rows)\n",
    "\n",
    "    return Decision_Node(question, true_branch, false_branch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = load_boston(return_X_y = True);\n",
    "data = load_boston();\n",
    "df = pd.DataFrame(data.data, columns = data.feature_names);\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QA54r4DiQDkM"
   },
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "  def __init__(self, X, y, n_trees, sample_sz, min_leaf = 2, max_depth=2):\n",
    "    self.root = None\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.n_trees = n_trees\n",
    "    self.sample_sz = sample_sz\n",
    "    self.min_leaf = min_leaf\n",
    "    self.max_depth = max_depth\n",
    "\n",
    "  def build_tree(self, X, y, level = 0):\n",
    "    num_samples, num_features = np.shape(dataframe)\n",
    "    n_labels = np.unique(y)\n",
    "\n",
    "    if level >= self.max_depth or n_labels == 1 or n_samples < self.min_leaf:\n",
    "        leaf = self.most_common_label(y)\n",
    "\n",
    "    features_idx = \n",
    "    best_feature, best_thresh = self.choose_best(X, y, )\n",
    "\n",
    "\n",
    "  def predict(self, X):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Make_Node():\n",
    "    def __init__(self, feature_idx = None, threshold = None, left = None, right = None, variance = None, value = None):\n",
    "        self.feature_idx = feature_idx\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.variance = variance\n",
    "        self.value = value #For leaf node\n",
    "\n",
    "class RegressionTree():\n",
    "    def __init__(self, max_depth, min_split):\n",
    "        self.root = None\n",
    "        self.min_split = min_split\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "\n",
    "    def split(self, df, feature_idx, threshold):\n",
    "        left_child = np.array([sample for sample in df if sample[feature_idx] <= threshold])\n",
    "        right_child = np.array([sample for sample in df if sample[feature_idx] > threshold])\n",
    "        return left_child, right_child\n",
    "\n",
    "    def train_squared_error(self, y, y_left, y_right):\n",
    "\n",
    "        weight_left = len(y_left) / len(y)\n",
    "        weight_right = len(y_right)/ len(y)\n",
    "        variance = weight_left * np.var(y_left) + weight_right * np.var(y_right)\n",
    "\n",
    "        #code says to subtract the parents error by the error fo the children. Not sure why we aren't just returning the variance of the children.\n",
    "        return variance \n",
    "\n",
    "\n",
    "\n",
    "    def get_leaf(self, y):\n",
    "        return np.mean(y) #presentation says to take the mode?\n",
    "\n",
    "    def best_feature(self, df, n_samples, n_features):\n",
    "        best_split_dict = {}\n",
    "        max_reduction = np.inf #Change variable to max_trained_squared_error?\n",
    "\n",
    "        for feat_idx in range(n_features):\n",
    "            feat_vals = df[:, feat_idx]\n",
    "            threshold_options = np.unique(feat_vals)\n",
    "\n",
    "            for threshold in threshold_options:\n",
    "                left_child, right_child = self.split(df, feat_idx, threshold)\n",
    "            \n",
    "                if len(left_child) > 0 and len(right_child) > 0:\n",
    "                    y, left_y, right_y = df[:, -1], left_child[:, -1], right_child[:, -1]\n",
    "\n",
    "                    trained_squared_error = self.train_squared_error(y, left_y, right_y)\n",
    "\n",
    "                    if trained_squared_error < max_reduction: #We want minimum variance. (If the trained_error function were to subtract frm the variance of the parent, flip to greater than).\n",
    "                            best_split_dict[\"feature_idx\"] = feat_idx\n",
    "                            best_split_dict[\"threshold\"] = threshold\n",
    "                            best_split_dict[\"left_child\"] = left_child\n",
    "                            best_split_dict[\"right_child\"] = right_child\n",
    "                            best_split_dict[\"variance\"] = trained_squared_error\n",
    "                            max_reduction = trained_squared_error\n",
    "        \n",
    "        return best_split_dict\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        df = np.concatenate((X, y), axis =1)\n",
    "        self.root = self.build_tree(df)\n",
    "\n",
    "    def build_tree(self, df, level = 0):\n",
    "        X, y = df[:, :-1], df[:,-1]\n",
    "        n_samples, n_features = np.shape(X)\n",
    "        best_split = {} #hold all features of the split in  dict.\n",
    "        if n_samples>= self.min_split and level <=self.max_depth:    #a1) Find the best feature for our tree to split on, if we haven't reached past our required tree size.\n",
    "\n",
    "            best_split = self.best_feature(df, n_samples, n_features)\n",
    "\n",
    "            if best_split['variance'] > 0:                          #a2) Split the tree based off this best feature into left and right subtrees\n",
    "                left_tree = self.build_tree(best_split['left_child'], level +1)\n",
    "                right_tree = self.build_tree(best_split['right_child'], level+1)\n",
    "\n",
    "                return Make_Node(best_split['feature_idx'], best_split['threshold'], left_tree, right_tree, best_split['variance']) #a3) Make current split into Parent Node\n",
    "        \n",
    "        leaf = self.get_leaf(y)                                      #b1) if tree size has already been reached, (or reduction = 0), then create and return a Leaf Node.\n",
    "\n",
    "        return Make_Node(value = leaf)\n",
    "\n",
    "\n",
    "    def print_tree(self, columns, tree=None, indent=\" \", ):\n",
    "            ''' function to print the tree '''\n",
    "            self.columns = columns\n",
    "\n",
    "            if not tree:\n",
    "                tree = self.root\n",
    "\n",
    "            if tree.value is not None:\n",
    "                print(tree.value)\n",
    "\n",
    "            else:\n",
    "                # if self.columns is None:\n",
    "                #     print(\"X_\"+str(tree.feature_idx), \"<=\", tree.threshold, \"?\", tree.variance)\n",
    "                    \n",
    "                # else:\n",
    "                print(\"X_\"+str(self.columns[tree.feature_idx]), \"<=\", tree.threshold, \"?\", tree.variance)\n",
    "\n",
    "                print(\"%sleft:\" % (indent), end=\"\")\n",
    "                self.print_tree(tree.left, indent + indent)\n",
    "                print(\"%sright:\" % (indent), end=\"\")\n",
    "                self.print_tree(tree.right, indent + indent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "Y = df.iloc[:, -1].values.reshape(-1,1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_INDUS <= 7.38 ? 32.287344445753966\n",
      " left:"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4480/1418849971.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mregressor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegressionTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4480/2375132406.py\u001b[0m in \u001b[0;36mprint_tree\u001b[1;34m(self, columns, tree, indent)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%sleft:\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mindent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%sright:\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mindent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4480/2375132406.py\u001b[0m in \u001b[0;36mprint_tree\u001b[1;34m(self, columns, tree, indent)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "regressor = RegressionTree(min_split=2, max_depth=2)\n",
    "regressor.fit(X_train, Y_train)\n",
    "regressor.print_tree(columns= df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4480/2194555055.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'yes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "columns = len(None)\n",
    "\n",
    "if columns:\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TF5TjNuvTKof"
   },
   "source": [
    "## Using Decision Treefor Digits Classification\n",
    "Remeber the MNIST dataset used - you will now test the power of decision trees on this problem.\n",
    "This time you are given a free hand in choosing the test and train set sizes, model parameters (such as gain function and constraints over the trees) and features (whether to use binary pixel values or the original continous gray value).\n",
    "- Choose which model parameters you wish to optimize, explain how would you do that, and find a model which you believe would have the minimal generalization error --- do this for both a single decision tree model, and a random forest.\n",
    "  - You can use `sklearn.tree.DecisionTreeClassifier`\n",
    "- Once you are satisfied with the model parameters, plot the importance of each of the pixels to the final decision.\n",
    "- Last, estimate the class assignment probabilities for all the correctly classified and misclassified examples in your test data. (confusion matrix)\n",
    "- Discuss your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8-k9WpIV_n7Y",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# code and answer go here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Decision Trees - Exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
